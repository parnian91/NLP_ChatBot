# NLP_ChatBot
* Memory networks are a specialized architecture that consist of a memory unit in addition to other learnable units, usually RNNs. Each input updates the memory state and the final output is computed by using the memory along with the output from the learnable unit. 
* This architecture was suggested in **2014** via the paper ***(Memory Networks, by J. Weston, S. Chopra, and A. Bordes, arXiv:1410.3916, 2014)***. A year later, another paper ***(Towards AIComplete Question Answering: A Set of Prerequisite Toy Tasks, by J. Weston, arXiv:1502.05698, 2015)*** put forward the idea of a synthetic dataset and a standard set of 20 question answering tasks, each with a higher degree of difficulty than the previous one, and applied various deep learning networks to solve these tasks. Of these, the memory network achieved the best results across all the tasks. 
* This dataset was later made available to the general public through **Facebook's bAbI** project (https://research.fb.com/projects/babi/). The implementation of our memory network resembles most closely the one described in this paper (***End-To-End Memory Networks, by S. Sukhbaatar, J. Weston, and R. Fergus, Advances in Neural Information Processing Systems, 2015)***, in that all the training happens jointly in a single network. It uses the bAbI dataset to solve the first question answering task.
